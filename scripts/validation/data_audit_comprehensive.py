#!/usr/bin/env python3
"""
SalesBoost æ•°æ®èµ„äº§å…¨é¢å®¡è®¡è„šæœ¬
Data Asset Comprehensive Audit Script

æ‰§è¡Œäº”ç»´åº¦æ•°æ®å®¡è®¡ï¼š
1. æ•°æ®å¯ç”¨æ€§éªŒè¯ä¸æŸåæ£€æµ‹
2. æ•°æ®åº”ç”¨åœºæ™¯æ˜ å°„
3. æ•°æ®ä»·å€¼é‡Šæ”¾æ–¹æ¡ˆ
4. æ•°æ®å‡çº§æ¼”è¿›è·¯å¾„
5. æ•°æ®ç¼ºå£è¯†åˆ«ä¸è·å–ç­–ç•¥
"""

import json
import hashlib
import mimetypes
from pathlib import Path
from typing import Dict, Any
from datetime import datetime

# æ•°æ®èµ„äº§è·¯å¾„é…ç½®
DATA_ASSETS = {
    "sales_knowledge_db": "é”€å† èƒ½åŠ›å¤åˆ¶æ•°æ®åº“",
    "intent_data": "data/intent",
    "storage": "storage",
    "tests_data": "tests/data"
}

class DataAssetAuditor:
    """æ•°æ®èµ„äº§å®¡è®¡å™¨"""

    def __init__(self, base_path: str = "/d/SalesBoost"):
        self.base_path = Path(base_path)
        self.audit_results = {
            "timestamp": datetime.now().isoformat(),
            "dimensions": {}
        }

    def dimension_1_availability_validation(self) -> Dict[str, Any]:
        """ç»´åº¦1: æ•°æ®å¯ç”¨æ€§éªŒè¯ä¸æŸåæ£€æµ‹"""
        print("\n=== ç»´åº¦1: æ•°æ®å¯ç”¨æ€§éªŒè¯ä¸æŸåæ£€æµ‹ ===")

        results = {
            "file_inventory": [],
            "integrity_checks": [],
            "quality_issues": [],
            "health_score": 0.0
        }

        # æ‰«æé”€å† èƒ½åŠ›å¤åˆ¶æ•°æ®åº“
        sales_db_path = self.base_path / DATA_ASSETS["sales_knowledge_db"]
        if sales_db_path.exists():
            for file_path in sales_db_path.rglob("*"):
                if file_path.is_file() and not file_path.name.startswith('.'):
                    file_info = self._analyze_file(file_path)
                    results["file_inventory"].append(file_info)

                    # å®Œæ•´æ€§æ£€æŸ¥
                    integrity = self._check_file_integrity(file_path, file_info)
                    results["integrity_checks"].append(integrity)

        # è®¡ç®—å¥åº·åº¦åˆ†æ•°
        total_files = len(results["file_inventory"])
        healthy_files = sum(1 for check in results["integrity_checks"] if check["status"] == "healthy")
        results["health_score"] = (healthy_files / total_files * 100) if total_files > 0 else 0

        # ç»Ÿè®¡
        results["summary"] = {
            "total_files": total_files,
            "healthy_files": healthy_files,
            "corrupted_files": total_files - healthy_files,
            "total_size_mb": sum(f["size_bytes"] for f in results["file_inventory"]) / (1024 * 1024)
        }

        print(f"[OK] æ‰«ææ–‡ä»¶: {total_files}")
        print(f"[OK] å¥åº·æ–‡ä»¶: {healthy_files}")
        print(f"[WARN]  é—®é¢˜æ–‡ä»¶: {total_files - healthy_files}")
        print(f"ğŸ“Š å¥åº·åº¦è¯„åˆ†: {results['health_score']:.2f}%")

        return results

    def dimension_2_application_mapping(self) -> Dict[str, Any]:
        """ç»´åº¦2: æ•°æ®åº”ç”¨åœºæ™¯æ˜ å°„"""
        print("\n=== ç»´åº¦2: æ•°æ®åº”ç”¨åœºæ™¯æ˜ å°„ ===")

        results = {
            "data_catalog": [],
            "agent_mapping_matrix": {},
            "permission_hierarchy": {}
        }

        # æ„å»ºæ•°æ®èµ„äº§æ¸…å•
        catalog_items = [
            {
                "asset_name": "äº§å“æƒç›Šæ•°æ®",
                "path": "é”€å† èƒ½åŠ›å¤åˆ¶æ•°æ®åº“/äº§å“æƒç›Š",
                "format": "XLSX",
                "use_cases": ["RAGæ£€ç´¢", "äº§å“çŸ¥è¯†åº“", "FAQç”Ÿæˆ"],
                "agents": ["CoachAgent", "ComplianceAgent"],
                "priority": "æ ¸å¿ƒæ•°æ®",
                "update_frequency": "æœˆåº¦"
            },
            {
                "asset_name": "é”€å”®è¯æœ¯SOP",
                "path": "é”€å† èƒ½åŠ›å¤åˆ¶æ•°æ®åº“/é”€å”®æˆäº¤è¥é”€SOPå’Œè¯æœ¯",
                "format": "PDF/DOCX/PPT",
                "use_cases": ["è¯æœ¯è®­ç»ƒ", "NPCå¯¹è¯ç”Ÿæˆ", "ç­–ç•¥åˆ†æ"],
                "agents": ["NPCGenerator", "StrategyAnalyzer", "CoachAgent"],
                "priority": "æ ¸å¿ƒæ•°æ®",
                "update_frequency": "å­£åº¦"
            },
            {
                "asset_name": "é”€å”®å½•éŸ³",
                "path": "é”€å† èƒ½åŠ›å¤åˆ¶æ•°æ®åº“/é”€å”®å½•éŸ³",
                "format": "MP3/WAV",
                "use_cases": ["è¯­éŸ³è¯†åˆ«è®­ç»ƒ", "å¯¹è¯æ¨¡å¼åˆ†æ", "æƒ…æ„Ÿåˆ†æ"],
                "agents": ["NPCGenerator", "FeedbackAgent"],
                "priority": "è¾…åŠ©æ•°æ®",
                "update_frequency": "å®æ—¶"
            },
            {
                "asset_name": "é”€å”®å† å†›ç»éªŒ",
                "path": "é”€å† èƒ½åŠ›å¤åˆ¶æ•°æ®åº“/é”€å”®å† å†›æˆäº¤ç»éªŒåˆ†äº«",
                "format": "DOCX",
                "use_cases": ["æœ€ä½³å®è·µæå–", "ç­–ç•¥æ¨è", "æ¡ˆä¾‹åº“"],
                "agents": ["StrategyAnalyzer", "ReportGenerator"],
                "priority": "æ ¸å¿ƒæ•°æ®",
                "update_frequency": "å­£åº¦"
            },
            {
                "asset_name": "æ„å›¾åˆ†ç±»è®­ç»ƒé›†",
                "path": "data/intent",
                "format": "CSV",
                "use_cases": ["æ„å›¾è¯†åˆ«æ¨¡å‹è®­ç»ƒ", "å¯¹è¯ç†è§£"],
                "agents": ["ContextAwareClassifier"],
                "priority": "æ ¸å¿ƒæ•°æ®",
                "update_frequency": "å‘¨åº¦"
            }
        ]

        results["data_catalog"] = catalog_items

        # æ„å»ºæ™ºèƒ½ä½“-æ•°æ®æ˜ å°„çŸ©é˜µ
        agent_mapping = {
            "CoachAgent": {
                "primary_data": ["äº§å“æƒç›Šæ•°æ®", "é”€å”®è¯æœ¯SOP"],
                "secondary_data": ["é”€å”®å† å†›ç»éªŒ"],
                "data_access_pattern": "å®æ—¶æ£€ç´¢",
                "cache_strategy": "LRUç¼“å­˜"
            },
            "NPCGenerator": {
                "primary_data": ["é”€å”®è¯æœ¯SOP", "é”€å”®å½•éŸ³"],
                "secondary_data": ["æ„å›¾åˆ†ç±»è®­ç»ƒé›†"],
                "data_access_pattern": "æ‰¹é‡åŠ è½½",
                "cache_strategy": "é¢„åŠ è½½"
            },
            "ComplianceAgent": {
                "primary_data": ["äº§å“æƒç›Šæ•°æ®"],
                "secondary_data": [],
                "data_access_pattern": "è§„åˆ™åŒ¹é…",
                "cache_strategy": "å…¨é‡ç¼“å­˜"
            },
            "StrategyAnalyzer": {
                "primary_data": ["é”€å”®å† å†›ç»éªŒ", "é”€å”®è¯æœ¯SOP"],
                "secondary_data": ["æ„å›¾åˆ†ç±»è®­ç»ƒé›†"],
                "data_access_pattern": "åˆ†æå‹æŸ¥è¯¢",
                "cache_strategy": "ç»“æœç¼“å­˜"
            }
        }

        results["agent_mapping_matrix"] = agent_mapping

        # æ•°æ®æƒé™åˆ†çº§
        results["permission_hierarchy"] = {
            "æ ¸å¿ƒæ•°æ®": {
                "access_level": "L1",
                "encryption": "AES-256",
                "backup_frequency": "æ¯æ—¥",
                "retention_period": "æ°¸ä¹…"
            },
            "è¾…åŠ©æ•°æ®": {
                "access_level": "L2",
                "encryption": "AES-128",
                "backup_frequency": "æ¯å‘¨",
                "retention_period": "1å¹´"
            },
            "ä¸´æ—¶æ•°æ®": {
                "access_level": "L3",
                "encryption": "æ— ",
                "backup_frequency": "æ— ",
                "retention_period": "30å¤©"
            }
        }

        print(f"[OK] æ•°æ®èµ„äº§æ¸…å•: {len(catalog_items)} é¡¹")
        print(f"[OK] æ™ºèƒ½ä½“æ˜ å°„: {len(agent_mapping)} ä¸ªæ™ºèƒ½ä½“")
        print(f"[OK] æƒé™åˆ†çº§: {len(results['permission_hierarchy'])} çº§")

        return results

    def dimension_3_value_release(self) -> Dict[str, Any]:
        """ç»´åº¦3: æ•°æ®ä»·å€¼é‡Šæ”¾æ–¹æ¡ˆ"""
        print("\n=== ç»´åº¦3: æ•°æ®ä»·å€¼é‡Šæ”¾æ–¹æ¡ˆ ===")

        results = {
            "multimodal_pipeline": {},
            "dynamic_loading": {},
            "monitoring_dashboard": {}
        }

        # å¤šæ¨¡æ€æ•°æ®å¤„ç†æµæ°´çº¿
        results["multimodal_pipeline"] = {
            "text_processing": {
                "formats": ["PDF", "DOCX", "XLSX", "TXT"],
                "tools": ["PyPDF2", "python-docx", "openpyxl"],
                "chunking_strategy": "semantic_chunking",
                "embedding_model": "BAAI/bge-m3",
                "vector_store": "Qdrant"
            },
            "audio_processing": {
                "formats": ["MP3", "WAV"],
                "tools": ["whisper", "librosa"],
                "transcription": "OpenAI Whisper",
                "speaker_diarization": "pyannote.audio",
                "emotion_analysis": "sentiment_analysis"
            },
            "structured_data": {
                "formats": ["CSV", "JSON", "XLSX"],
                "tools": ["pandas", "polars"],
                "validation": "pydantic",
                "normalization": "sklearn.preprocessing"
            }
        }

        # åŠ¨æ€æ•°æ®åŠ è½½æœºåˆ¶
        results["dynamic_loading"] = {
            "lazy_loading": {
                "strategy": "æŒ‰éœ€åŠ è½½",
                "cache_size": "1GB",
                "eviction_policy": "LRU"
            },
            "incremental_update": {
                "change_detection": "æ–‡ä»¶å“ˆå¸Œå¯¹æ¯”",
                "update_frequency": "5åˆ†é’Ÿ",
                "hot_reload": True
            },
            "version_control": {
                "enabled": True,
                "storage": "Git LFS",
                "rollback_support": True
            }
        }

        # æ•°æ®æ•ˆæœç›‘æ§ä½“ç³»
        results["monitoring_dashboard"] = {
            "usage_metrics": [
                "æ•°æ®è®¿é—®é¢‘ç‡",
                "ç¼“å­˜å‘½ä¸­ç‡",
                "æŸ¥è¯¢å“åº”æ—¶é—´",
                "æ•°æ®æ–°é²œåº¦"
            ],
            "business_metrics": [
                "RAGæ£€ç´¢å‡†ç¡®ç‡",
                "å¯¹è¯æˆåŠŸç‡",
                "ç”¨æˆ·æ»¡æ„åº¦",
                "è½¬åŒ–ç‡æå‡"
            ],
            "alerting": {
                "data_staleness": "> 7å¤©",
                "cache_miss_rate": "> 30%",
                "query_latency": "> 500ms"
            }
        }

        print(f"[OK] å¤šæ¨¡æ€å¤„ç†: {len(results['multimodal_pipeline'])} ç§æ•°æ®ç±»å‹")
        print(f"[OK] åŠ¨æ€åŠ è½½: {len(results['dynamic_loading'])} ç§æœºåˆ¶")
        print(f"[OK] ç›‘æ§æŒ‡æ ‡: {len(results['monitoring_dashboard']['usage_metrics']) + len(results['monitoring_dashboard']['business_metrics'])} é¡¹")

        return results

    def dimension_4_evolution_path(self) -> Dict[str, Any]:
        """ç»´åº¦4: æ•°æ®å‡çº§æ¼”è¿›è·¯å¾„"""
        print("\n=== ç»´åº¦4: æ•°æ®å‡çº§æ¼”è¿›è·¯å¾„ ===")

        results = {
            "version_management": {},
            "data_augmentation": {},
            "migration_plan": {}
        }

        # æ•°æ®ç‰ˆæœ¬ç®¡ç†
        results["version_management"] = {
            "versioning_scheme": "Semantic Versioning (v1.2.3)",
            "datasets": {
                "training_data": {
                    "current_version": "v1.0.0",
                    "storage": "data/intent/v1.0.0/",
                    "changelog": "åˆå§‹ç‰ˆæœ¬"
                },
                "test_data": {
                    "current_version": "v1.0.0",
                    "storage": "data/intent/test/v1.0.0/",
                    "changelog": "åˆå§‹ç‰ˆæœ¬"
                },
                "validation_data": {
                    "current_version": "v1.0.0",
                    "storage": "data/intent/validation/v1.0.0/",
                    "changelog": "åˆå§‹ç‰ˆæœ¬"
                }
            },
            "tools": ["DVC (Data Version Control)", "Git LFS"]
        }

        # æ•°æ®å¢å¼ºç­–ç•¥
        results["data_augmentation"] = {
            "synonym_expansion": {
                "method": "WordNet + é¢†åŸŸè¯å…¸",
                "target_increase": "3x",
                "quality_threshold": 0.85
            },
            "sample_balancing": {
                "method": "SMOTE + æ¬ é‡‡æ ·",
                "target_distribution": "å‡åŒ€åˆ†å¸ƒ",
                "minority_class_threshold": 50
            },
            "time_series_completion": {
                "method": "æ’å€¼ + é¢„æµ‹æ¨¡å‹",
                "missing_data_handling": "å‰å‘å¡«å……",
                "validation": "äº¤å‰éªŒè¯"
            },
            "llm_generation": {
                "model": "DeepSeek-V3",
                "use_cases": ["å¯¹è¯æ ·æœ¬ç”Ÿæˆ", "FAQæ‰©å±•", "åœºæ™¯æ¨¡æ‹Ÿ"],
                "quality_filter": "äººå·¥å®¡æ ¸ + è‡ªåŠ¨è¯„åˆ†"
            }
        }

        # æ•°æ®è¿ç§»æ–¹æ¡ˆ
        results["migration_plan"] = {
            "phase_1_assessment": {
                "duration": "1å‘¨",
                "tasks": ["æ•°æ®ä¾èµ–åˆ†æ", "å…¼å®¹æ€§æµ‹è¯•", "é£é™©è¯„ä¼°"]
            },
            "phase_2_preparation": {
                "duration": "2å‘¨",
                "tasks": ["æ•°æ®å¤‡ä»½", "è¿ç§»è„šæœ¬å¼€å‘", "å›æ»šæ–¹æ¡ˆåˆ¶å®š"]
            },
            "phase_3_execution": {
                "duration": "1å‘¨",
                "tasks": ["ç°åº¦è¿ç§»", "æ•°æ®éªŒè¯", "æ€§èƒ½æµ‹è¯•"]
            },
            "phase_4_validation": {
                "duration": "1å‘¨",
                "tasks": ["ä¸šåŠ¡éªŒè¯", "ç›‘æ§å‘Šè­¦", "æ–‡æ¡£æ›´æ–°"]
            },
            "rollback_strategy": {
                "trigger_conditions": ["æ•°æ®ä¸¢å¤±", "æ€§èƒ½ä¸‹é™>20%", "ä¸šåŠ¡ä¸­æ–­"],
                "rollback_time": "< 30åˆ†é’Ÿ",
                "data_recovery": "ä»å¤‡ä»½æ¢å¤"
            }
        }

        print(f"[OK] ç‰ˆæœ¬ç®¡ç†: {len(results['version_management']['datasets'])} ä¸ªæ•°æ®é›†")
        print(f"[OK] å¢å¼ºç­–ç•¥: {len(results['data_augmentation'])} ç§æ–¹æ³•")
        print(f"[OK] è¿ç§»é˜¶æ®µ: {len(results['migration_plan']) - 1} ä¸ªé˜¶æ®µ")

        return results

    def dimension_5_gap_identification(self) -> Dict[str, Any]:
        """ç»´åº¦5: æ•°æ®ç¼ºå£è¯†åˆ«ä¸è·å–ç­–ç•¥"""
        print("\n=== ç»´åº¦5: æ•°æ®ç¼ºå£è¯†åˆ«ä¸è·å–ç­–ç•¥ ===")

        results = {
            "data_gaps": [],
            "acquisition_strategies": {},
            "evaluation_framework": {}
        }

        # è¯†åˆ«æ•°æ®ç¼ºå£
        results["data_gaps"] = [
            {
                "gap_type": "ç«å“å¯¹æ¯”æ•°æ®ä¸è¶³",
                "impact": "é«˜",
                "current_coverage": "30%",
                "target_coverage": "90%",
                "priority": "P0"
            },
            {
                "gap_type": "å®¢æˆ·å¼‚è®®å¤„ç†æ¡ˆä¾‹",
                "impact": "é«˜",
                "current_coverage": "40%",
                "target_coverage": "85%",
                "priority": "P0"
            },
            {
                "gap_type": "é•¿å°¾åœºæ™¯å¯¹è¯æ•°æ®",
                "impact": "ä¸­",
                "current_coverage": "20%",
                "target_coverage": "70%",
                "priority": "P1"
            },
            {
                "gap_type": "å®æ—¶å¸‚åœºåŠ¨æ€",
                "impact": "ä¸­",
                "current_coverage": "10%",
                "target_coverage": "80%",
                "priority": "P1"
            },
            {
                "gap_type": "å¤šè¯­è¨€æ”¯æŒæ•°æ®",
                "impact": "ä½",
                "current_coverage": "5%",
                "target_coverage": "60%",
                "priority": "P2"
            }
        ]

        # å¤šæºæ•°æ®è·å–æ–¹æ¡ˆ
        results["acquisition_strategies"] = {
            "web_scraping": {
                "targets": ["ç«å“å®˜ç½‘", "é‡‘èè®ºå›", "ç¤¾äº¤åª’ä½“"],
                "tools": ["Scrapy", "BeautifulSoup", "Selenium"],
                "anti_scraping": {
                    "user_agent_rotation": True,
                    "proxy_pool": True,
                    "rate_limiting": "1 req/s",
                    "captcha_solving": "2Captcha API"
                },
                "legal_compliance": {
                    "robots_txt_check": True,
                    "terms_of_service_review": True,
                    "data_usage_rights": "ä»…ç”¨äºç ”ç©¶å’Œè®­ç»ƒ"
                }
            },
            "llm_generation": {
                "model": "DeepSeek-V3",
                "use_cases": [
                    "å¯¹è¯åœºæ™¯ç”Ÿæˆ",
                    "FAQæ‰©å±•",
                    "å¼‚è®®å¤„ç†è¯æœ¯"
                ],
                "quality_filter": {
                    "automatic_scoring": {
                        "relevance": "> 0.8",
                        "coherence": "> 0.85",
                        "diversity": "> 0.7"
                    },
                    "human_review": {
                        "sample_rate": "10%",
                        "acceptance_threshold": "> 90%"
                    }
                },
                "cost_estimation": {
                    "tokens_per_sample": 500,
                    "cost_per_1k_tokens": "$0.001",
                    "target_samples": 10000,
                    "total_cost": "$5"
                }
            },
            "synthetic_data": {
                "methods": [
                    "CTGAN (è¡¨æ ¼æ•°æ®)",
                    "VAE (æ—¶é—´åºåˆ—)",
                    "GAN (å¯¹è¯ç”Ÿæˆ)"
                ],
                "validation": {
                    "distribution_similarity": "KLæ•£åº¦ < 0.1",
                    "statistical_tests": "Kolmogorov-Smirnovæ£€éªŒ",
                    "domain_expert_review": True
                },
                "use_cases": [
                    "å°‘æ ·æœ¬åœºæ™¯æ‰©å……",
                    "éšç§ä¿æŠ¤æ•°æ®ç”Ÿæˆ",
                    "è¾¹ç¼˜æ¡ˆä¾‹æ¨¡æ‹Ÿ"
                ]
            },
            "crowdsourcing": {
                "platforms": ["Amazon MTurk", "å›½å†…ä¼—åŒ…å¹³å°"],
                "tasks": [
                    "å¯¹è¯æ ‡æ³¨",
                    "æ„å›¾åˆ†ç±»",
                    "è´¨é‡è¯„ä¼°"
                ],
                "quality_control": {
                    "worker_qualification": "é€šè¿‡ç‡ > 95%",
                    "redundancy": "3äººæ ‡æ³¨",
                    "gold_standard": "10%é»„é‡‘æ ‡å‡†é¢˜"
                }
            }
        }

        # æ•°æ®è·å–æ•ˆæœè¯„ä¼°ä½“ç³»
        results["evaluation_framework"] = {
            "cost_metrics": {
                "acquisition_cost_per_sample": "ç›®æ ‡: < $0.1",
                "processing_cost": "ç›®æ ‡: < $0.05",
                "storage_cost": "ç›®æ ‡: < $0.01/GB/æœˆ"
            },
            "quality_metrics": {
                "data_accuracy": "ç›®æ ‡: > 95%",
                "coverage_improvement": "ç›®æ ‡: +50%",
                "model_performance_gain": "ç›®æ ‡: +10% F1-score"
            },
            "business_metrics": {
                "time_to_value": "ç›®æ ‡: < 2å‘¨",
                "roi": "ç›®æ ‡: > 300%",
                "user_satisfaction_increase": "ç›®æ ‡: +15%"
            }
        }

        print(f"[OK] æ•°æ®ç¼ºå£: {len(results['data_gaps'])} ä¸ª")
        print(f"[OK] è·å–ç­–ç•¥: {len(results['acquisition_strategies'])} ç§")
        print(f"[OK] è¯„ä¼°æŒ‡æ ‡: {len(results['evaluation_framework'])} ç±»")

        return results

    def _analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        stat = file_path.stat()
        mime_type, _ = mimetypes.guess_type(str(file_path))

        return {
            "path": str(file_path.relative_to(self.base_path)),
            "name": file_path.name,
            "extension": file_path.suffix,
            "size_bytes": stat.st_size,
            "size_human": self._human_readable_size(stat.st_size),
            "mime_type": mime_type or "unknown",
            "modified_time": datetime.fromtimestamp(stat.st_mtime).isoformat(),
            "checksum": self._calculate_checksum(file_path)
        }

    def _check_file_integrity(self, file_path: Path, file_info: Dict) -> Dict[str, Any]:
        """æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§"""
        status = "healthy"
        issues = []

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        if file_info["size_bytes"] == 0:
            status = "corrupted"
            issues.append("æ–‡ä»¶å¤§å°ä¸º0")

        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åä¸MIMEç±»å‹åŒ¹é…
        if file_info["mime_type"] == "unknown":
            status = "warning"
            issues.append("æ— æ³•è¯†åˆ«æ–‡ä»¶ç±»å‹")

        # æ£€æŸ¥ä¸´æ—¶æ–‡ä»¶
        if file_path.name.startswith('.~') or file_path.name.startswith('~$'):
            status = "warning"
            issues.append("ä¸´æ—¶æ–‡ä»¶")

        return {
            "file": file_info["path"],
            "status": status,
            "issues": issues
        }

    def _calculate_checksum(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶MD5æ ¡éªŒå’Œ"""
        try:
            md5 = hashlib.md5()
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    md5.update(chunk)
            return md5.hexdigest()
        except Exception:
            return "error"

    def _human_readable_size(self, size_bytes: int) -> str:
        """è½¬æ¢ä¸ºäººç±»å¯è¯»çš„æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.2f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.2f} TB"

    def generate_report(self) -> str:
        """ç”Ÿæˆå®Œæ•´å®¡è®¡æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("SalesBoost æ•°æ®èµ„äº§å…¨é¢å®¡è®¡æŠ¥å‘Š")
        print("="*80)

        # æ‰§è¡Œäº”ä¸ªç»´åº¦çš„å®¡è®¡
        self.audit_results["dimensions"]["dimension_1"] = self.dimension_1_availability_validation()
        self.audit_results["dimensions"]["dimension_2"] = self.dimension_2_application_mapping()
        self.audit_results["dimensions"]["dimension_3"] = self.dimension_3_value_release()
        self.audit_results["dimensions"]["dimension_4"] = self.dimension_4_evolution_path()
        self.audit_results["dimensions"]["dimension_5"] = self.dimension_5_gap_identification()

        # ä¿å­˜æŠ¥å‘Š
        report_path = self.base_path / "DATA_ASSET_AUDIT_REPORT.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.audit_results, f, ensure_ascii=False, indent=2)

        print(f"\n[OK] å®¡è®¡æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        return str(report_path)


def main():
    """ä¸»å‡½æ•°"""
    auditor = DataAssetAuditor()
    report_path = auditor.generate_report()

    print("\n" + "="*80)
    print("å®¡è®¡å®Œæˆï¼")
    print("="*80)
    print(f"\nğŸ“Š å®Œæ•´æŠ¥å‘Š: {report_path}")
    print("\nä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
    print("1. å®¡æŸ¥æ•°æ®è´¨é‡é—®é¢˜å¹¶ä¿®å¤")
    print("2. å®æ–½æ•°æ®å¢å¼ºç­–ç•¥")
    print("3. éƒ¨ç½²åŠ¨æ€æ•°æ®åŠ è½½æœºåˆ¶")
    print("4. å¯åŠ¨æ•°æ®ç¼ºå£å¡«è¡¥è®¡åˆ’")
    print("5. å»ºç«‹æ•°æ®ç›‘æ§ä»ªè¡¨æ¿")


if __name__ == "__main__":
    main()
